{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "little-silver",
   "metadata": {},
   "source": [
    "## Saurabh Dadhich \n",
    "### Indian Institute Of Technology, Roorkee"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "selective-watershed",
   "metadata": {},
   "source": [
    "##  FaceMask Detection:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "european-romania",
   "metadata": {},
   "source": [
    "### Importing required packages: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "surprising-suspension",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.applications.mobilenet_v2 import preprocess_input\n",
    "from tensorflow.keras.preprocessing.image import img_to_array\n",
    "from tensorflow.keras.models import load_model\n",
    "import numpy as np\n",
    "import imutils\n",
    "import time\n",
    "import cv2\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "periodic-lindsay",
   "metadata": {},
   "source": [
    "### Function returning location of face and predictions: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "digital-jacket",
   "metadata": {},
   "outputs": [],
   "source": [
    "def detect_and_predict_mask(frame, faceNet, maskNet):\n",
    "    (h, w) = frame.shape[:2]\n",
    "    \n",
    "    #cv2.dnn.blobFromImage(image, scalefactor=1.0, size, mean, swapRB=True)  \n",
    "    #-- return blob after mean subtraction, normalizing, and channel swapping.\n",
    "    blob = cv2.dnn.blobFromImage(frame, 1.0, (224, 224),(104.0, 177.0, 123.0))\n",
    "   # print(\"Blob shape: \",blob.shape)             # (1, 3, 224, 224)\n",
    "    \n",
    "    faceNet.setInput(blob)\n",
    "    detections = faceNet.forward()\n",
    "   # print(\"FaceNet output shape: \",detections.shape)\n",
    "    \n",
    "    faces = []\n",
    "    locs = []\n",
    "    preds = []\n",
    "\n",
    "    for i in range(detections.shape[2]):\n",
    "        # Probability associated with detection\n",
    "        confidence = detections[0, 0, i, 2]\n",
    "        \n",
    "        # detections[0,0,i,0] = 1 for face not found     -- Box not defined \n",
    "        # detection[0,0,i,1]  = 1 for face found  \n",
    "        \n",
    "        # Defining threshold to be 0.5\n",
    "        if confidence>0.5:\n",
    "            # Computing box:\n",
    "          #  print(\"detections place:  \",detections[0,0,i]) \n",
    "            \n",
    "            box = detections[0, 0, i, 3:7] * np.array([w, h, w, h])\n",
    "            (startX, startY, endX, endY) = box.astype(\"int\")\n",
    "            \n",
    "            # ensure the bounding boxes fall within the dimensions of the frame\n",
    "            (startX, startY) = (max(0, startX), max(0, startY))\n",
    "            (endX, endY) = (min(w - 1, endX), min(h - 1, endY))\n",
    "            \n",
    "\n",
    "            # extract the face, convert it from BGR to RGB channel\n",
    "            # ordering, resize it to 224x224, and preprocess it\n",
    "            face = frame[startY:endY, startX:endX]\n",
    "            face = cv2.cvtColor(face, cv2.COLOR_BGR2RGB) \n",
    "            face = cv2.resize(face, (224, 224))\n",
    "            face = img_to_array(face)  \n",
    "            face = preprocess_input(face)\n",
    "            \n",
    "            faces.append(face)\n",
    "            locs.append((startX, startY, endX, endY))\n",
    "            \n",
    "            \n",
    "    if len(faces) > 0:\n",
    "            # for faster inference we'll make batch predictions on *all*\n",
    "            # faces at the same time rather than one-by-one predictions \n",
    "            \n",
    "            faces = np.array(faces, dtype=\"float32\")\n",
    "            preds = maskNet.predict(faces, batch_size=32)\n",
    "\n",
    "    \n",
    "    \n",
    "    # return a 2-tuple of the face locations and their corresponding locations\n",
    "    return (locs, preds)         "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "exotic-architect",
   "metadata": {},
   "source": [
    "### Loading face detector model: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "harmful-memphis",
   "metadata": {},
   "outputs": [],
   "source": [
    "prototxtPath = r\"F:\\Spark foundation\\FaceMask Detector\\face_detector_model\\deploy.prototxt\"\n",
    "weightsPath = r\"F:\\Spark foundation\\FaceMask Detector\\face_detector_model\\res10_300x300_ssd_iter_140000.caffemodel\"\n",
    "faceNet = cv2.dnn.readNet(prototxtPath, weightsPath)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "scenic-supplement",
   "metadata": {},
   "source": [
    "### Loading mask detection model: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "naval-gasoline",
   "metadata": {},
   "outputs": [],
   "source": [
    "maskNet = load_model(r\"F:\\Spark foundation\\FaceMask Detector\\Mask_detector_model\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "tracked-marriage",
   "metadata": {},
   "source": [
    "### Video capture from Web camera: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "announced-clear",
   "metadata": {},
   "outputs": [],
   "source": [
    "cap = cv2.VideoCapture(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "handy-queue",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "detections place:   [0.         1.         0.70062953 0.36716807 0.46710122 0.6014607\n",
      " 0.8476614 ]\n",
      "detections place:   [0.         1.         0.947679   0.37929457 0.49874926 0.6240377\n",
      " 0.8921641 ]\n",
      "detections place:   [0.        1.        0.8998814 0.383971  0.5074719 0.6237159 0.8902409]\n",
      "detections place:   [0.         1.         0.7695738  0.38054544 0.49221265 0.6204197\n",
      " 0.90582335]\n",
      "detections place:   [0.         1.         0.7695738  0.38054544 0.49221265 0.6204197\n",
      " 0.90582335]\n",
      "detections place:   [0.         1.         0.7982419  0.38807473 0.51010066 0.6293512\n",
      " 0.9008605 ]\n",
      "detections place:   [0.         1.         0.75132936 0.40283793 0.5026115  0.6340361\n",
      " 0.89364266]\n",
      "detections place:   [0.         1.         0.5830169  0.40524003 0.5117115  0.63275516\n",
      " 0.9033816 ]\n",
      "detections place:   [0.         1.         0.5560817  0.41433316 0.5075011  0.6407538\n",
      " 0.90536547]\n",
      "detections place:   [0.         1.         0.7522373  0.4016484  0.5159924  0.63093257\n",
      " 0.9029213 ]\n",
      "detections place:   [0.         1.         0.64475113 0.4108193  0.5091225  0.63848424\n",
      " 0.89685476]\n",
      "detections place:   [0.         1.         0.8477438  0.43527278 0.4963659  0.65911883\n",
      " 0.9025675 ]\n",
      "detections place:   [0.         1.         0.7890243  0.4477722  0.51447576 0.6633999\n",
      " 0.8846763 ]\n",
      "detections place:   [0.         1.         0.77496046 0.44546533 0.5070087  0.6668625\n",
      " 0.886641  ]\n",
      "detections place:   [0.         1.         0.7695033  0.44306344 0.5007873  0.66728944\n",
      " 0.8803242 ]\n",
      "detections place:   [0.         1.         0.9555706  0.43909597 0.49580395 0.65553486\n",
      " 0.8557612 ]\n",
      "detections place:   [0.         1.         0.6304378  0.4363     0.49077034 0.6590581\n",
      " 0.84686327]\n",
      "detections place:   [0.         1.         0.80460674 0.43206972 0.49154103 0.6527334\n",
      " 0.8566011 ]\n",
      "detections place:   [0.         1.         0.80460674 0.43206972 0.49154103 0.6527334\n",
      " 0.8566011 ]\n",
      "detections place:   [0.         1.         0.92561466 0.42172185 0.49081025 0.6406592\n",
      " 0.84678805]\n",
      "detections place:   [0.         1.         0.751155   0.4129996  0.49377102 0.6359128\n",
      " 0.85918003]\n",
      "detections place:   [0.         1.         0.76559204 0.4149983  0.4991648  0.6443807\n",
      " 0.86685896]\n",
      "detections place:   [0.         1.         0.7409583  0.4222017  0.5063214  0.6389157\n",
      " 0.85205907]\n",
      "detections place:   [0.         1.         0.9419613  0.4118939  0.50159425 0.63925904\n",
      " 0.87046224]\n",
      "detections place:   [0.         1.         0.7243376  0.4199778  0.49716723 0.64044267\n",
      " 0.8750435 ]\n",
      "detections place:   [0.         1.         0.8962284  0.42144072 0.49760318 0.6375023\n",
      " 0.8720511 ]\n",
      "detections place:   [0.         1.         0.8962284  0.42144072 0.49760318 0.6375023\n",
      " 0.8720511 ]\n",
      "detections place:   [0.         1.         0.54209083 0.42428324 0.5008147  0.6468154\n",
      " 0.87977934]\n",
      "detections place:   [0.         1.         0.6757918  0.43871573 0.4874928  0.66549677\n",
      " 0.8864224 ]\n",
      "detections place:   [0.         1.         0.8616553  0.44227484 0.49664602 0.6795128\n",
      " 0.8811362 ]\n",
      "detections place:   [0.         1.         0.9468967  0.44976005 0.49576047 0.6778537\n",
      " 0.8882172 ]\n",
      "detections place:   [0.         1.         0.9395777  0.45201918 0.4985478  0.6778246\n",
      " 0.88220704]\n"
     ]
    }
   ],
   "source": [
    "while True:\n",
    "    # Capturing frame:\n",
    "    ret, frame = cap.read()\n",
    "    \n",
    "    # imutils.resize() only tries to evaluate either the width or height argument,\n",
    "    # and tries to preserve the original aspect ratio of the image,\n",
    "    # while cv2.resize() gives you a free choice for height and width.\n",
    "    frame = imutils.resize(frame, width=400)\n",
    "\n",
    "    (locs, preds) = detect_and_predict_mask(frame, faceNet, maskNet)\n",
    "    \n",
    "    for (box, pred) in zip(locs, preds):\n",
    "        # unpack the bounding box and predictions\n",
    "        (startX, startY, endX, endY) = box\n",
    "        (mask, withoutMask) = pred\n",
    "\n",
    "        # determine the class label and color we'll use to draw the bounding box and text\n",
    "        label = \"Mask\" if mask > withoutMask else \"No Mask\"\n",
    "        color = (0, 255, 0) if label == \"Mask\" else (0, 0, 255)\n",
    "\n",
    "        # include the probability in the label\n",
    "        label = \"{}: {:.2f}%\".format(label, max(mask, withoutMask) * 100)\n",
    "\n",
    "        cv2.putText(frame, label, (startX, startY - 10),cv2.FONT_HERSHEY_SIMPLEX, 0.45, color, 2)\n",
    "        cv2.rectangle(frame, (startX, startY), (endX, endY), color, 2)\n",
    "        \n",
    "    cv2.imshow(\"Frame\",frame)\n",
    "    key = cv2.waitKey(1)\n",
    "\n",
    "    # if the `ESC` key was pressed, break from the loop\n",
    "    if key == 27:    \n",
    "        break\n",
    "  \n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aerial-operation",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "future-consultation",
   "metadata": {},
   "source": [
    "#### END"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
